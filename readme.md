## fnbr.vision

### 1. papers :page_facing_up:

BELCAVELLO, F.; VIRIDIANO, M.; DINIZ DA COSTA, A.; MATOS, E. E.; TORRENT, T. T. (2020). Frame-Based Annotation of Multimodal Corpora: Tracking (A)Synchronies in Meaning Construction. In: **Proceedings of the LREC International FrameNet Workshop 2020**. Marseille, France: ELRA, p. 23-30. [![bibtex](https://img.shields.io/static/v1?label=&message=bibtex&color=blue)](https://www.aclweb.org/anthology/2020.framenet-1.4.bib) [![pdf](https://img.shields.io/static/v1?label=&message=pdf&color=red)](https://www.aclweb.org/anthology/2020.framenet-1.4.pdf)

* [My Zotero library.](https://www.zotero.org/viridiano/library)

### 2. books :books:

Study the fundamentals first by reading Speech and Language Processing, 2nd Edition, by Jurafsky and Martin. [The 3rd edition is in progress and some chapters are available as pdf](https://web.stanford.edu/~jurafsky/slp3/).

Also...

* GOLDBERG, Yoav. Neural network methods for natural language processing. **Synthesis Lectures on Human Language Technologies,** v. 10, n. 1, p. 1-309, 2017. ![doi] `10.2200/S00762ED1V01Y201703HLT037` *(you know what to do)*
* HUTCHINS, William John; SOMERS, Harold L. **An introduction to machine translation.** London: Academic Press, 1992. [[download pdf]](http://www.hutchinsweb.me.uk/IntroMT-TOC.htm)
* MANNING, Christopher D.; MANNING, Christopher D.; SCHÜTZE, Hinrich. **Foundations of statistical natural language processing.** MIT press, 1999. [[download pdf]](http://thuvien.thanglong.edu.vn:8081/dspace/bitstream/DHTL_123456789/4027/1/cs511-1.pdf)
* KOEHN, Philipp. **Neural machine translation.** arXiv preprint arXiv:1709.07809, 2017. [[download pdf]](https://arxiv.org/pdf/1709.07809.pdf)
* KOEHN, Philipp. **Statistical machine translation.** Cambridge University Press, 2009. ![doi] `10.1017/CBO9780511815829`

The following texts are useful, but not required. All of them can be read free online.

* Yoav Goldberg. [A Primer on Neural Network Models for Natural Language Processing](http://u.cs.biu.ac.il/~yogo/nnlp.pdf)
* Ian Goodfellow, Yoshua Bengio, and Aaron Courville. [Deep Learning](http://www.deeplearningbook.org)

If you have no background in Neural Networks, you might well find one of these books helpful to give you more background:

* Michael A. Nielsen. [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com)
* Eugene Charniak. [Introduction to Deep Learning](https://mitpress.mit.edu/books/introduction-deep-learning)

For learning about Deep Learning for NLP, take the [Stanford cs224n online course](http://web.stanford.edu/class/cs224n/) or watch the [Stanford cs224n Lecture collection on NLP with Deep Learning](https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6).

### 3. lectures :speech_balloon:

* [Dan Jurafsky and Christopher Manning: Introduction to NLP](https://www.youtube.com/playlist?list=PLQiyVNMpDLKnZYBTUOlSI9mi9wAErFtFm) :new:
* [Stanford cs231n: Detection and Segmentation](https://youtu.be/nDPWywWRIRo) | [lecture slides](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)
* [Stanford cs224n: Lecture collection on NLP with Deep Learning](https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6)

### 4. repos :octocat:

* [NLP Progress](https://github.com/sebastianruder/NLP-progress) *– Repository to track the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks, by [@sebastianruder](https://github.com/sebastianruder)*

### 5. blog posts :pushpin:

* [A complete overview of ML online courses](https://www.freecodecamp.org/news/every-single-machine-learning-course-on-the-internet-ranked-by-your-reviews-3c4a7b8026c0) *– Every single Machine Learning course on the internet, ranked by your reviews*
* [An overview of semantic image segmentation](https://www.jeremyjordan.me/semantic-segmentation/) *– how to use convolutional neural networks for the task of semantic image segmentation*
* [Going beyond the bounding box with semantic segmentation](https://thegradient.pub/semantic-segmentation/)
* [Semantic Image Segmentation with DeepLab in TensorFlow](https://ai.googleblog.com/2018/03/semantic-image-segmentation-with.html)

### 6. tools :hammer:

* [YOLOv3](https://pjreddie.com/darknet/yolo/) *– Real-Time Object Detection*
* [labelme](https://github.com/wkentaro/labelme) *– Image Polygonal Annotation with Python*
* [Figure Eight](https://www.figure-eight.com) *– If you need labels and annotations for your machine learning project, we can help. You upload your unlabeled data, with the rules you need for your machine learning project, and launch. We use a distributed network of human annotators and cutting edge machine learning models to annotate that data at enterprise scale*

### 7. datasets :cloud:

| Dataset Download                  | Paper          | Description |
| :-------------------------------- | :------------- | :---------- |
| [Multi 30K](https://github.com/multi30k/dataset)   | [Elliott et al. 2016] [arXiv:1605.00459](https://arxiv.org/abs/1605.00459) |  Extends the Flickr30K dataset with German translations created by professional translators over a subset of the English descriptions |
| [Flickr 30K Entities](https://github.com/BryanPlummer/flickr30k_entities) | [Plummer et al. 2015] [arXiv:1505.04870](https://arxiv.org/abs/1505.04870) | 244k coreference chains and 276k manually annotated bounding boxes for each of the 31,783 images and 158,915 English captions (five per image) in the original dataset |
| [Flickr 30K](http://shannon.cs.illinois.edu/DenotationGraph/data/index.html) | [Young et al. 2014] ![doi] [10.1162/tacl_a_00166](https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00166) | Standard benchmark for sentence-based image description |
| [MS COCO](http://cocodataset.org/) | [Lin et al. 2014] [arXiv:1405.0312](https://arxiv.org/abs/1405.0312) | Large-scale object detection, segmentation, and captioning dataset |
| [AVA](https://research.google.com/ava/index.html) | [Roth et al. 2019] [arXiv:1901.01342](https://arxiv.org/abs/1901.01342) | Spatio-temporal audiovisual annotations of human actions in movies, suitable for training localized action recognition systems |
| [Open Images](https://storage.googleapis.com/openimages/web/index.html) | [Kuznetsova et al. 2018] [arXiv:1811.00982](https://arxiv.org/abs/1811.00982) | ~9M images annotated with image-level labels, object bounding boxes, object segmentation masks, visual relationships, and localized narrative |

Also...

* [YouTube BoundingBoxes](https://research.google.com/youtube-bb/) *– Large-scale data set of video URLs with densely-sampled high-quality single-object bounding box annotations. All the video segments were human-annotated with high precision classifications and bounding boxes at 1 frame per second.*
* [What's Cookin'](http://storage.googleapis.com/whats_cookin/whats_cookin.zip) *– A list of cooking-related Youtube video ids, along with time stamps marking the (estimated) start and end of various events.*
* [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/) *– A standardised image data sets for object class recognition and a common set of tools for accessing the data sets and annotations*
* [PASCAL Context](https://www.cs.stanford.edu/~roozbeh/pascal-context/) *– Indoor and outdoor scenes with 400+ classes*
* [MPII Human Pose Dataset](http://human-pose.mpi-inf.mpg.de/) *– State of the art benchmark for evaluation of articulated human pose estimation*
* [Cityscapes Dataset](https://www.cityscapes-dataset.com) *– benchmark suite and evaluation server for pixel-level and instance-level semantic labeling*
* [Mapillary Vistas Dataset](https://www.mapillary.com/dataset/vistas) *– a diverse street-level imagery dataset with pixel‑accurate and instance‑specific human annotations for understanding street scenes around the world*
* [ApolloScape Scene Parsing](http://apolloscape.auto/scene.html) *– RGB videos with high resolution image sequences and per pixel annotation, survey-grade dense 3D points with semantic segmentation*
* [Stanford Background Dataset](http://dags.stanford.edu/projects/scenedataset.html) *– A set of outdoor scenes with at least one foreground object*

### 8. semantic parsers :ballot_box_with_check:

* [SEMAFOR](http://www.cs.cmu.edu/~ark/SEMAFOR/) *– automatically processes English sentences according to the form of semantic analysis in Berkeley FrameNet.*
* [Google Sling](https://github.com/google/sling) *– Natural language frame semantics parser*
* [Open Sesame](https://github.com/swabhs/open-sesame) *– Frame-semantic parsing system based on a softmax-margin SegRNN*
* [PathLSMT](https://github.com/microth/PathLSTM) *– Neural SRL model*

[doi]:http://viridiano.com/s/doi_16x16.png

## fnbr.vision

### 1. papers

- SILBERER, Carina; PINKAL, Manfred. Grounding semantic roles in images. In: **Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.** 2018. p. 2616-2626. [[download PDF]](https://www.aclweb.org/anthology/D18-1282)
- GUPTA, Saurabh; MALIK, Jitendra. Visual semantic role labeling. **arXiv preprint arXiv:1505.04474,** 2015. [[download PDF]](https://arxiv.org/pdf/1505.04474.pdf)
- YANG, Shaohua et al. Grounded semantic role labeling. In: **Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.** 2016. p. 149-159. [[download PDF]](https://www.aclweb.org/anthology/N16-1019)
- YATSKAR, Mark; ZETTLEMOYER, Luke; FARHADI, Ali. Situation recognition: Visual semantic role labeling for image understanding. In: **Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.** 2016. p. 5534-5542. [[download PDF]](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yatskar_Situation_Recognition_Visual_CVPR_2016_paper.pdf)
- LONG, Jonathan; SHELHAMER, Evan; DARRELL, Trevor. Fully convolutional networks for semantic segmentation. In: **Proceedings of the IEEE conference on computer vision and pattern recognition.** 2015. p. 3431-3440. [[dowload pdf]](https://arxiv.org/pdf/1605.06211.pdf)

### 2. books


The following texts are useful, but not required. All of them can be read free online.


- Dan Jurafsky and James H. Martin. [Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/)
- Yoav Goldberg. [A Primer on Neural Network Models for Natural Language Processing](http://u.cs.biu.ac.il/~yogo/nnlp.pdf)
- Ian Goodfellow, Yoshua Bengio, and Aaron Courville. [Deep Learning](http://www.deeplearningbook.org)

If you have no background in neural networks, you might well find one of these books helpful to give you more background:

- Michael A. Nielsen. [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com)
- Eugene Charniak. [Introduction to Deep Learning](https://mitpress.mit.edu/books/introduction-deep-learning)

### 3. lectures

- [Stanford cs231n: Detection and Segmentation](https://youtu.be/nDPWywWRIRo) | [lecture slides](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)
- [Stanford cs224n: Lecture collection on NLP with Deep Learning](https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6)

### 4. blog posts

- [An overview of semantic image segmentation](https://www.jeremyjordan.me/semantic-segmentation/) *– how to use convolutional neural networks for the task of semantic image segmentation*
- [Going beyond the bounding box with semantic segmentation](https://thegradient.pub/semantic-segmentation/)
- [Semantic Image Segmentation with DeepLab in TensorFlow](https://ai.googleblog.com/2018/03/semantic-image-segmentation-with.html)

### 5. tools

- [labelme](https://github.com/wkentaro/labelme) *– Image Polygonal Annotation with Python*

### 6. datasets

- [COCO](http://cocodataset.org) *– Common Objects in Context*
- [Cityscapes Dataset](https://www.cityscapes-dataset.com) *– benchmark suite and evaluation server for pixel-level and instance-level semantic labeling*
- [Mapillary Vistas Dataset](https://www.mapillary.com/dataset/vistas) *– a diverse street-level imagery dataset with pixel‑accurate and instance‑specific human annotations for understanding street scenes around the world*
- [ApolloScape Scene Parsing](http://apolloscape.auto/scene.html) *– RGB videos with high resolution image sequences and per pixel annotation, survey-grade dense 3D points with semantic segmentation*
_____

- ![Low][low] means that the item has a **low** priority.
- ![Medium][medium] means that the item has a **medium** priority. You shouldn't avoid tackling that item.
- ![High][high] means that the item has a **high** priority. You can't avoid that item.


[low]: https://viridiano.com/s/low.svg
[medium]: https://viridiano.com/s/medium.svg
[high]: https://viridiano.com/s/high.svg

## fnbr.vision

### 1. papers :page_facing_up:
###### Use Zotero? [Go to my library.](https://www.zotero.org/viridiano/library)

* ABEND, Omri; RAPPOPORT, Ari. The state of the art in semantic representation. In: **Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).** 2017. p. 77-89. [[download pdf]](https://www.aclweb.org/anthology/P17-1008)
* BHAGAT, P. K.; CHOUDHARY, Prakash. Image annotation: Then and now. **Image and Vision Computing,** v. 80, p. 1-23, 2018. [[download pdf]](https://www.sciencedirect.com/science/article/pii/S0262885618301628)
* BONIAL, Claire et al. PropBank: Semantics of New Predicate Types. In: **LREC.** 2014. p. 3013-3019. [[download pdf]](https://pdfs.semanticscholar.org/0f18/060a4c86a3fd81c4c01cc3de55b902dec08d.pdf)
* CHAI, Joyce; PAN, Shimei; ZHOU, Michelle X. MIND: A Semantics-based multimodal interpretation framework for conversational systems. In: **Proceedings of the International Workshop on Natural, Intelligent and Effective Interaction in Multimodal Dialogue Systems.** Springer-Verlag, 2002. p. 37-46. [[download pdf]](https://www.researchgate.net/profile/Malek_Boualem/publication/243962636_Implementing_and_evaluating_a_multimodal_and_multilingual_tourist_guide/links/557c110f08aeea18b7766620/Implementing-and-evaluating-a-multimodal-and-multilingual-tourist-guide.pdf#page=49) ![multimodality](https://img.shields.io/static/v1?label=&message=multimodality&color=green)
* COHN, Neil. A multimodal parallel architecture: A cognitive framework for multimodal interactions. **Cognition**, v. 146, p. 304-323, 2016. [[download pdf]](https://doi.org/10.1016/j.cognition.2015.10.007) ![multimodality](https://img.shields.io/static/v1?label=&message=multimodality&color=green)
* DINIZ DA COSTA, A.; GAMONAL, M. A.; PAIVA, V. M. R. L.; MARÇÃO, N. D.; PERON-CORRÊA, S.; ALMEIDA, V. G.; MATOS, E. E. S.; TORRENT, T. T. (2018). In: **Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)**, Miyazaki, Japan: ELRA, p. 6-12. [[download pdf]](http://lrec-conf.org/workshops/lrec2018/W5/pdf/4_W5.pdf) ![frmsemantics](https://img.shields.io/static/v1?label=&message=frm_semantics&color=blue)
* ELLIOTT, Desmond et al. Findings of the second shared task on multimodal machine translation and multilingual image description. **arXiv preprint arXiv:1710.07177,** 2017. [[download pdf]](https://arxiv.org/pdf/1710.07177.pdf) ![multimodality](https://img.shields.io/static/v1?label=&message=multimodality&color=green)
* FILLMORE, Charles J. An alternative to checklist theories of meaning. In: **Annual Meeting of the Berkeley Linguistics Society.** 1975. p. 123-131. [[dowload pdf]](http://journals.linguisticsociety.org/proceedings/index.php/BLS/article/download/2315/2085) ![frmsemantics](https://img.shields.io/static/v1?label=&message=frm_semantics&color=blue)
* FILLMORE, Charles J. et al. Frame semantics. **Cognitive linguistics: Basic readings**, v. 34, p. 373-400, 2006. [[dowload pdf]](https://s3.amazonaws.com/academia.edu.documents/56930879/Cognitive_Linguistics__Basic_Readings.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1556653987&Signature=cqR7QalZlj%2BdnCRz%2FKVECRys2TE%3D&response-content-disposition=inline%3B%20filename%3DCognitive_Linguistics_Basic_Readings.pdf#page=381) ![frmsemantics](https://img.shields.io/static/v1?label=&message=frm_semantics&color=blue)
* FILLMORE, Charles J. The case for case reopened. **Syntax and semantics**, v. 8, n. 1977, p. 59-82, 1977.[[download pdf]](http://www.icsi.berkeley.edu/pubs/ai/casefor277.pdf) ![frmsemantics](https://img.shields.io/static/v1?label=&message=frm_semantics&color=blue)
* FILLMORE, C. J., PETRUCK, M. R., RUPPENHOFER, J., & WRIGHT, A. (2003). FrameNet in action: The case of attaching. **International journal of lexicography**, 16(3), 297-332. [[download pdf]](https://core.ac.uk/download/pdf/83654409.pdf) ![frmsemantics](https://img.shields.io/static/v1?label=&message=frm_semantics&color=blue)
* FILLMORE, C. J., & ATKINS, B. T. (1992). Toward a frame-based lexicon: The semantics of RISK and its neighbors. In: **Frames, fields, and contrasts: New essays in semantic and lexical organization**, 103, 75-102. [[download pdf]](www.icsi.berkeley.edu/pubs/ai/towarda92.pdf) ![frmsemantics](https://img.shields.io/static/v1?label=&message=frm_semantics&color=blue)
* GUPTA, Saurabh; MALIK, Jitendra. Visual semantic role labeling. **arXiv preprint arXiv:1505.04474,** 2015. [[download pdf]](https://arxiv.org/pdf/1505.04474.pdf)  ![vsrl](https://img.shields.io/static/v1?label=&message=vsrl&color=orange)
* LALA, Chiraag; SPECIA, Lucia. Multimodal lexical translation. In: **proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).** 2018. [[download pdf]](https://www.aclweb.org/anthology/L18-1602.pdf) ![multimodality](https://img.shields.io/static/v1?label=&message=multimodality&color=green)
* LONG, Jonathan; SHELHAMER, Evan; DARRELL, Trevor. Fully convolutional networks for semantic segmentation. In: **Proceedings of the IEEE conference on computer vision and pattern recognition.** 2015. p. 3431-3440. [[dowload pdf]](https://arxiv.org/pdf/1605.06211.pdf)
* MARTINEC, Radan; SALWAY, Andrew. A system for image–text relations in new (and old) media. **Visual communication**, v. 4, n. 3, p. 337-371, 2005. [[download pdf]](https://journals.sagepub.com/doi/pdf/10.1177/1470357205055928)
* MCKEVITT, Paul. MultiModal semantic representation. **SIGSEM Working Group on the Representation of MultiModal Semantic Information**, p. 1-16, 2003. [[download pdf]](http://uir.ulster.ac.uk/21402/1/msr.pdf)
* REIMERINK, Arianne; DE QUESADA, Mercedes García; MONTERO-MARTÍNEZ, Silvia. Contextual information in terminological knowledge bases: A multimodal approach. **Journal of pragmatics**, v. 42, n. 7, p. 1928-1950, 2010. [[download pdf]](https://s3.amazonaws.com/academia.edu.documents/48477241/Contextual_information_in_terminological20160831-25332-9lzmdl.pdf?response-content-disposition=inline%3B%20filename%3DContextual_information_in_terminological.pdf&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWOWYYGZ2Y53UL3A%2F20191105%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20191105T125216Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=0b5f4913427d7adc1b20cadf71cdd079505f5cbfd0c4f45be92b3240a62b860b) ![multimodality](https://img.shields.io/static/v1?label=&message=multimodality&color=green)
* ROYCE, Terry; ROYCE, T. D.; BOWCHER, W. L. Intersemiotic complementarity: a framework for multimodal. **New directions in the analysis of multimodal discourse**, p. 63-109, 2007. [[download pdf]](https://s3.amazonaws.com/academia.edu.documents/32865031/Chap_02_Royce_-_Erlbaum.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1559668579&Signature=mkcmdOBsxWK1tnIxgdwLmONLM48%3D&response-content-disposition=inline%3B%20filename%3DIntersemiotic_Complementarity_A_Framewor.pdf) ![multimodality](https://img.shields.io/static/v1?label=&message=multimodality&color=green)
* SHAH, Kashif; WANG, J. K.; SPECIA, Lucia. Shef-multimodal: Grounding machine translation on images. In: **Proceedings of the First Conference on Machine Translation.** ACL, 2016. p. 660-665. [[download pdf]](http://eprints.whiterose.ac.uk/102140/1/wmt2016mmt.pdf)  ![multimodality](https://img.shields.io/static/v1?label=&message=multimodality&color=green)
* SILBERER, Carina; PINKAL, Manfred. Grounding semantic roles in images. In: **Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.** 2018. p. 2616-2626. [[download pdf]](https://www.aclweb.org/anthology/D18-1282) ![vsrl](https://img.shields.io/static/v1?label=&message=vsrl&color=orange) ![multimodality](https://img.shields.io/static/v1?label=&message=multimodality&color=green)
* SANABRIA, Ramon et al. How2: a large-scale dataset for multimodal language understanding. **arXiv preprint arXiv:1811.00347,** 2018. [[download pdf]](https://arxiv.org/pdf/1811.00347.pdf) ![multimodality](https://img.shields.io/static/v1?label=&message=multimodality&color=green)
* SPECIA, Lucia et al. A shared task on multimodal machine translation and crosslingual image description. In: **Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers.** 2016. p. 543-553. [[download pdf]](https://www.aclweb.org/anthology/W16-2346.pdf) ![multimodality](https://img.shields.io/static/v1?label=&message=multimodality&color=green)
* TORRET, T. T., DA SILVA MATOS, E. E., LAGE, L., LAVIOLA, A., DA SILVA TAVARES, T., DE ALMEIDA, V. G., & SIGILIANO, N. (2018). Towards continuity between the lexicon and the constructicon in FrameNet Brasil. In Lyngfelt, B.; Borin, L.; Ohara, K.; Torrent, T. T. **Constructicography: Constructicon development across languages** (pp. 107-140). Amsterdam: John Benjamins. [[download pdf]](https://www.jbe-platform.com/content/books/9789027263865)
* TORRENT, T. T.; ELLSWORTH, M.; BAKER, C. F.; MATOS, E. E. (2018). In: **Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)**, Miyazaki, Japan: ELRA, p. 62-68. [[download pdf]](http://lrec-conf.org/workshops/lrec2018/W5/pdf/12_W5.pdf)
* YANG, Shaohua et al. Grounded semantic role labeling. In: **Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.** 2016. p. 149-159. [[download pdf]](https://www.aclweb.org/anthology/N16-1019) ![vsrl](https://img.shields.io/static/v1?label=&message=vsrl&color=orange)
* YATSKAR, Mark; ZETTLEMOYER, Luke; FARHADI, Ali. Situation recognition: Visual semantic role labeling for image understanding. In: **Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.** 2016. p. 5534-5542. [[download pdf]](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yatskar_Situation_Recognition_Visual_CVPR_2016_paper.pdf) ![vsrl](https://img.shields.io/static/v1?label=&message=vsrl&color=orange)

### 2. books :books:

* GOLDBERG, Yoav. Neural network methods for natural language processing. **Synthesis Lectures on Human Language Technologies,** v. 10, n. 1, p. 1-309, 2017.
* HUTCHINS, William John; SOMERS, Harold L. **An introduction to machine translation.** London: Academic Press, 1992.
* MANNING, Christopher D.; MANNING, Christopher D.; SCHÜTZE, Hinrich. **Foundations of statistical natural language processing.** MIT press, 1999. [[download pdf]](http://thuvien.thanglong.edu.vn:8081/dspace/bitstream/DHTL_123456789/4027/1/cs511-1.pdf)
* KOEHN, Philipp. **Neural machine translation.** arXiv preprint arXiv:1709.07809, 2017. [[download pdf]](https://arxiv.org/pdf/1709.07809.pdf)
* KOEHN, Philipp. **Statistical machine translation.** Cambridge University Press, 2009.

*The following texts are useful, but not required. All of them can be read free online.*

* Dan Jurafsky and James H. Martin. [Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/)
* Yoav Goldberg. [A Primer on Neural Network Models for Natural Language Processing](http://u.cs.biu.ac.il/~yogo/nnlp.pdf)
* Ian Goodfellow, Yoshua Bengio, and Aaron Courville. [Deep Learning](http://www.deeplearningbook.org)

*If you have no background in neural networks, you might well find one of these books helpful to give you more background:*

* Michael A. Nielsen. [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com)
* Eugene Charniak. [Introduction to Deep Learning](https://mitpress.mit.edu/books/introduction-deep-learning)

### 3. lectures :speech_balloon:

* [Dan Jurafsky and Christopher Manning: Introduction to NLP](https://www.youtube.com/playlist?list=PLQiyVNMpDLKnZYBTUOlSI9mi9wAErFtFm) :new:
* [Stanford cs231n: Detection and Segmentation](https://youtu.be/nDPWywWRIRo) | [lecture slides](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)
* [Stanford cs224n: Lecture collection on NLP with Deep Learning](https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6)

### 4. blog posts :pushpin:

* [An overview of semantic image segmentation](https://www.jeremyjordan.me/semantic-segmentation/) *– how to use convolutional neural networks for the task of semantic image segmentation*
* [Going beyond the bounding box with semantic segmentation](https://thegradient.pub/semantic-segmentation/)
* [Semantic Image Segmentation with DeepLab in TensorFlow](https://ai.googleblog.com/2018/03/semantic-image-segmentation-with.html)

### 5. tools :hammer:

* [YOLOv3](https://pjreddie.com/darknet/yolo/) *– Real-Time Object Detection*
* [labelme](https://github.com/wkentaro/labelme) *– Image Polygonal Annotation with Python*
* [Figure Eight](https://www.figure-eight.com) *– If you need labels and annotations for your machine learning project, we can help. You upload your unlabeled data, with the rules you need for your machine learning project, and launch. We use a distributed network of human annotators and cutting edge machine learning models to annotate that data at enterprise scale*

### 6. datasets :cloud:

* [AVA](https://research.google.com/ava/index.html) *– Spatio-temporal annotations of human actions in movies, suitable for training localized action recognition systems.* :new:
* [YouTube BoundingBoxes](https://research.google.com/youtube-bb/) *– Large-scale data set of video URLs with densely-sampled high-quality single-object bounding box annotations. All the video segments were human-annotated with high precision classifications and bounding boxes at 1 frame per second.* :new:
* [What's Cookin'](http://storage.googleapis.com/whats_cookin/whats_cookin.zip) *– A list of cooking-related Youtube video ids, along with time stamps marking the (estimated) start and end of various events.* :new:
* [COCO](http://cocodataset.org) *– Common Objects in Context: a large-scale object detection, segmentation, and captioning dataset.*
* [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/) *– A standardised image data sets for object class recognition and a common set of tools for accessing the data sets and annotations*
* [PASCAL Context](https://www.cs.stanford.edu/~roozbeh/pascal-context/) *– Indoor and outdoor scenes with 400+ classes*
* [MPII Human Pose Dataset](http://human-pose.mpi-inf.mpg.de/) *– State of the art benchmark for evaluation of articulated human pose estimation*
* [Cityscapes Dataset](https://www.cityscapes-dataset.com) *– benchmark suite and evaluation server for pixel-level and instance-level semantic labeling*
* [Mapillary Vistas Dataset](https://www.mapillary.com/dataset/vistas) *– a diverse street-level imagery dataset with pixel‑accurate and instance‑specific human annotations for understanding street scenes around the world*
* [ApolloScape Scene Parsing](http://apolloscape.auto/scene.html) *– RGB videos with high resolution image sequences and per pixel annotation, survey-grade dense 3D points with semantic segmentation*
* [Stanford Background Dataset](http://dags.stanford.edu/projects/scenedataset.html) *– A set of outdoor scenes with at least one foreground object*

### 7. semantic parsers :ballot_box_with_check:

* [Google Sling](https://github.com/google/sling) *– Natural language frame semantics parser*
* [Open Sesame](https://github.com/swabhs/open-sesame) *– Frame-semantic parsing system based on a softmax-margin SegRNN*
* [PathLSMT](https://github.com/microth/PathLSTM) *– Neural SRL model*
* [SEMAFOR](http://www.cs.cmu.edu/~ark/SEMAFOR/)
